{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec0115e8550accd446b24d4ffae279ce",
     "grade": false,
     "grade_id": "Introduction",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this assignment, we will implement Domain-Adversarial Training of Neural Networks on SVHN$\\rightarrow$MNIST task. The [DANN](https://arxiv.org/pdf/1505.07818.pdf) is a popular unsupervised Domain Adaptation paper which uses the principle of adversarial learning to align the source and target datasets. A classifier trained on the source data can now classify the target data effectively. \n",
    "\n",
    "In this assignment we will treat the SVHN dataset of digits as the source domain and the MNIST dataset of digits as the target domain. Note that both the domains need to have the same categories. \n",
    "\n",
    "**It will be ideal to solve this assignemnet on a computer with a GPU**. The Coursera platform does not support a GPU. You may want to explore [Google Colab](https://www.youtube.com/watch?v=inN8seMm7UI&ab_channel=TensorFlow) or [Kaggle](https://www.kaggle.com/dansbecker/running-kaggle-kernels-with-a-gpu)\n",
    "\n",
    "Along with submitting the Python notebook, save the notebook along with its output after executing all the cells as a .html file and submit the html file as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad6fd31a7054ab94d1b3d18366861a65",
     "grade": false,
     "grade_id": "Imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch import optim\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import numpy.testing as npt\n",
    "import random\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from digits import get_mnist\n",
    "from digits import get_svhn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aae75656c24910c648e320c3b0f08a41",
     "grade": false,
     "grade_id": "Seed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## The following random seeds are just for deterministic behaviour of the code and evaluation\n",
    "\n",
    "##############################################################################\n",
    "################### DO NOT MODIFY THE CODE BELOW #############################    \n",
    "##############################################################################\n",
    "manual_seed = 0\n",
    "\n",
    "random.seed(manual_seed)\n",
    "np.random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(manual_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    \n",
    "############################################################################### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d4c6408e6d35abb6f4d17de419faa27",
     "grade": false,
     "grade_id": "Hyperparameters",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "#is_cuda=False\n",
    "print(\"GPU available: \" + str(is_cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d8420e62560a71f8b715ad80a6564a2",
     "grade": false,
     "grade_id": "cell-444f44762e793d6a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Download Source Data\n",
    "\n",
    "We will not use DataLoaders for this assignment becasue the dataset is small and it is faster to train with the dataset loaded in the memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getRGB = True\n",
    "src_trX, src_trY, src_tsX, src_tsY = get_svhn(getRGB=getRGB)\n",
    "#m,_,_,_ = src_trX.shape\n",
    "#tr_idx = np.random.choice(m,min(m,TRAIN_SAMPLES_TO_USE))\n",
    "#src_trX = src_trX[tr_idx,:,:,:]\n",
    "#src_trY = src_trY[tr_idx]\n",
    "#m,_,_,_ = src_tsX.shape\n",
    "#ts_idx = np.random.choice(m,min(m,TEST_SAMPLES_TO_USE))\n",
    "#src_tsX = src_tsX[ts_idx,:,:,:]\n",
    "#src_tsY = src_tsY[ts_idx]\n",
    "print('Src Train Min: Value- ',np.min(src_trX))\n",
    "print('Src Train Max: Value- ',np.max(src_trX))\n",
    "print('Src Test Min: Value- ',np.min(src_tsX))\n",
    "print('Src Test Max: Value- ',np.max(src_tsX))\n",
    "print('src_trX.shape: ', src_trX.shape)\n",
    "print('src_trY.shape: ', src_trY.shape)\n",
    "print('src_tsX.shape: ', src_tsX.shape)\n",
    "print('src_tsY.shape: ', src_tsY.shape)\n",
    "\n",
    "#Let's visualize few samples and their labels from the train and test dataset.\n",
    "if getRGB:\n",
    "    # For RGB svhn\n",
    "    visx_tr = src_trX[:50,:,:,:].reshape(5,10,3,32,32).transpose(0,3,1,4,2).reshape(32*5,-1,3)\n",
    "    visx_ts = src_tsX[:50,:,:,:].reshape(5,10,3,32,32).transpose(0,3,1,4,2).reshape(32*5,-1,3)\n",
    "    visx = np.concatenate((visx_tr,visx_ts), axis=0)\n",
    "    visx = (visx+1)/2. #scaling back to [0-1]\n",
    "else:\n",
    "    # For grayscale svhn\n",
    "    visx_tr = src_trX[:50,:,:,:].squeeze().reshape(5,10,32,32).transpose(0,2,1,3).reshape(32*5,-1)\n",
    "    visx_ts = src_tsX[:50,:,:,:].squeeze().reshape(5,10,32,32).transpose(0,2,1,3).reshape(32*5,-1)\n",
    "    visx = np.concatenate((visx_tr,visx_ts), axis=0)\n",
    "\n",
    "visy = np.concatenate((src_trY[:50],src_tsY[:50])).reshape(10,-1)\n",
    "print('labels')\n",
    "print(visy)\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.axis('off')\n",
    "if getRGB:\n",
    "    plt.imshow(visx)\n",
    "else:\n",
    "    plt.imshow(visx,cmap='gray')\n",
    "\n",
    "#convert to torch tensor\n",
    "src_trX = torch.tensor(src_trX)\n",
    "src_trY = torch.tensor(src_trY)\n",
    "src_tsX = torch.tensor(src_tsX)\n",
    "src_tsY = torch.tensor(src_tsY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2fa5bfa75f89eda3e7c484bc5cb0149e",
     "grade": false,
     "grade_id": "cell-5d2004a1c9a824f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Download Target Data\n",
    "\n",
    "We will not use DataLoaders for this assignment becasue the dataset is small and it is faster to train with the dataset loaded in the memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getRGB = True\n",
    "setSizeTo32 = False\n",
    "size = 32 if setSizeTo32 else 28\n",
    "tgt_trX, tgt_trY, tgt_tsX, tgt_tsY = get_mnist(getRGB=getRGB, setSizeTo32=setSizeTo32)\n",
    "#m,_,_,_ = tgt_trX.shape\n",
    "#tr_idx = np.random.choice(m,min(m,TRAIN_SAMPLES_TO_USE))\n",
    "#tgt_trX = tgt_trX[tr_idx,:,:,:]\n",
    "#tgt_trY = tgt_trY[tr_idx]\n",
    "#m,_,_,_ = tgt_tsX.shape\n",
    "#ts_idx = np.random.choice(m,min(m,TEST_SAMPLES_TO_USE))\n",
    "#tgt_tsX = tgt_tsX[ts_idx,:,:,:]\n",
    "#tgt_tsY = tgt_tsY[ts_idx]\n",
    "print('Tgt Train Min: Value- ',np.min(tgt_trX))\n",
    "print('Tgt Train Max: Value- ',np.max(tgt_trX))\n",
    "print('Tgt Test Min: Value- ',np.min(tgt_tsX))\n",
    "print('Tgt Test Max: Value- ',np.max(tgt_tsX))\n",
    "print('tgt_trX.shape: ', tgt_trX.shape)\n",
    "print('tgt_trY.shape: ', tgt_trY.shape)\n",
    "print('tgt_tsX.shape: ', tgt_tsX.shape)\n",
    "print('tgt_tsY.shape: ', tgt_tsY.shape)\n",
    "\n",
    "\n",
    "#Let's visualize few samples and their labels from the train and test dataset.\n",
    "if getRGB:\n",
    "    # For RGB svhn\n",
    "    visx_tr = tgt_trX[:50,:,:,:].reshape(5,10,3,size,size).transpose(0,3,1,4,2).reshape(size*5,-1,3)\n",
    "    visx_ts = tgt_tsX[:50,:,:,:].reshape(5,10,3,size,size).transpose(0,3,1,4,2).reshape(size*5,-1,3)\n",
    "    visx = np.concatenate((visx_tr,visx_ts), axis=0)\n",
    "    visx = (visx+1)/2. #scaling back to [0-1]\n",
    "else:\n",
    "    # For grayscale svhn\n",
    "    visx_tr = tgt_trX[:50,:,:,:].squeeze().reshape(5,10,size,size).transpose(0,2,1,3).reshape(size*5,-1)\n",
    "    visx_ts = tgt_tsX[:50,:,:,:].squeeze().reshape(5,10,size,size).transpose(0,2,1,3).reshape(size*5,-1)\n",
    "    visx = np.concatenate((visx_tr,visx_ts), axis=0)\n",
    "\n",
    "visy = np.concatenate((tgt_trY[:50],tgt_tsY[:50])).reshape(10,-1)\n",
    "print('labels')\n",
    "print(visy)\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.axis('off')\n",
    "if getRGB:\n",
    "    plt.imshow(visx)\n",
    "else:\n",
    "    plt.imshow(visx,cmap='gray')\n",
    "\n",
    "#convert to torch tensor\n",
    "tgt_trX = torch.tensor(tgt_trX)\n",
    "tgt_trY = torch.tensor(tgt_trY)\n",
    "tgt_tsX = torch.tensor(tgt_tsX)\n",
    "tgt_tsY = torch.tensor(tgt_tsY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2aa1069bced0ce7a1f80aa3832fea987",
     "grade": false,
     "grade_id": "cell-9d19fda38cd5cd6e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Design the Network Modules (20 points)\n",
    "\n",
    "Let's define the network architecures: \n",
    "Convolution Layers are represented as 'Conv(ch)', where 'ch' is the number of output channels. All convolutions are same conolutions with stride=3 and padding=1\n",
    "Linear layers are represented as 'Linear(fts)', where 'fts' is the number of output features. \n",
    "Every Convolution and Linear Layer is followed by ReLU activation except if it is the last layer before the Loss function.\n",
    "MaxPool layers are represented as 'MaxPool(fs)' where 'fs'=2 is the filter size with stride=2\n",
    "BatchNorm layers are represented as 'BatchNorm(ch)', where 'ch' is the number of channels or inputs feature dimensions. For Convolution layers we use 'BacthNorm2d(ch)' and for linear layers we use 'BatchNorm1d(ch)'\n",
    "\n",
    "FeatrureExtractor (Input = Batch_Size x 3 x 32 x 32 or Batch_Size x 3 x 28 x 28):<br>\n",
    "    Conv(32) $\\rightarrow$ Conv(32) $\\rightarrow$ MaxPool(2) $\\rightarrow$ BatchNorm(32) $\\rightarrow$ Conv(64) $\\rightarrow$ Conv(64) $\\rightarrow$ MaxPool(2) $\\rightarrow$ BatchNorm(64) $\\rightarrow$ Conv(128) $\\rightarrow$ Conv(128) $\\rightarrow$ AdaptiveAvgPool2d(1) $\\rightarrow$ Linear(128) $\\rightarrow$ BatchNorm(128) <br><br>\n",
    "LabelClassifier (Input = Batch_Size x 128): <br>\n",
    "    Linear(64) $\\rightarrow$ BatchNorm $\\rightarrow$ Linear(64) $\\rightarrow$ SoftmaxCrossEntropyLoss<br><br>\n",
    "DomainClassifier (Input = Batch_Size x 128): <br>\n",
    "     Linear(64) $\\rightarrow$ BatchNorm $\\rightarrow$ Linear(64) $\\rightarrow$ BatchNorm $\\rightarrow$ Linear(64) $\\rightarrow$ SigmoidBinaryCrossEntropyLoss<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94458731d16b80e6d894ea18e89b5b36",
     "grade": false,
     "grade_id": "Define_networks_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Feel free to modify the architecture within certain constraints, to get better results. \n",
    "# Please check the comments for the testcases to understand the constraints on the network architecture. \n",
    "\n",
    "# The FeatrureExtractor network module is used to extract features from the source and the target data.\n",
    "# Every image is extracted into a feature of 128 dimensions\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        # Define the components of the FeatrureExtractor\n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "        # Set up the weight initialization mechanishm \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward propagate through the FeatrureExtractor network module\n",
    "        # your code here\n",
    "        \n",
    "        return x\n",
    "\n",
    "# The LabelClassifier network module is used to classify the features (output of FeatureExtractor) into 10 categories\n",
    "class LabelClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LabelClassifier, self).__init__()\n",
    "        # Define the components of the LabelClassifier \n",
    "        # DO NOT define the loss layer. We will define it later\n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "        # Set up the weight initialization mechanishm \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward propagate through the LabelClassifier network module\n",
    "        # your code here\n",
    "        \n",
    "        return x\n",
    "\n",
    "# The following class is meant to reverse the gradient during backpropagation \n",
    "# It has been defined - no changes needed\n",
    "class GradReverse(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lamda):\n",
    "        ctx.lamda = lamda\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = (grad_output.neg() * ctx.lamda)\n",
    "        return output, None\n",
    "\n",
    "# The DomainClassifier module trains to distinguish between the source and target features \n",
    "# The input to the network are the features from the FeatureExtractor\n",
    "class DomainClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DomainClassifier, self).__init__()\n",
    "        # Define the components of the DomainClassifier \n",
    "        # DO NOT define the loss layer. We will define it later\n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "        # Set up the weight initialization mechanishm \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "\n",
    "    def forward(self, x, lam=0.0):\n",
    "        # Forward propagate through the LabelClassifier network module\n",
    "        # The Gradreverse has been implemented. \n",
    "        # Implement the other forward propoagation for the remianing componenets of the DomainClassifier\n",
    "        x = GradReverse.apply(x, lam)\n",
    "        # your code here\n",
    "        \n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2249a07ab83c66743b41df5a4158554",
     "grade": true,
     "grade_id": "Define_networks",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Testcases\n",
    "# The input to the FeatureExtractor is a Tensor (m,3,s,s), where m is number of samples (arbitrary batch_size)\n",
    "#        3 is the RGB channels, s is the image size. The FeatureExtractor has an AdaptiveAvgPool2d() layer \n",
    "#        feeding a fully connected layer. It is therefore not constarined to a specific image size. It works for both 28 and 32 \n",
    "# The output of the FeatureExtractor is a Tensor (m,n), where m is number of samples (arbitrary batch_size) \n",
    "#        and n=128 is feature dimensions. \n",
    "\n",
    "# The input to the FeatureClassifier is a Tensor (m,n), where m is number of samples (arbitrary batch_size) \n",
    "#        and n=128 is feature dimensions\n",
    "# The output of the FeatureClassifier is a Tensor (m,c), where m is number of samples (arbitrary batch_size) \n",
    "#        and c=10 is number of categories\n",
    "\n",
    "# The input to the DomainClassifier is a Tensor (m,n), where m is number of samples (arbitrary batch_size) \n",
    "#        and n=128 is feature dimensions\n",
    "# The output of the DomainClassifier is a Tensor (m,1), where m is number of samples (arbitrary batch_size) \n",
    "#        The values are between [0-1]\n",
    "\n",
    "# The following testcases test only for the constraints above. \n",
    "# You are free to select your own architecture to get better results.\n",
    "\n",
    "f_t = FeatureExtractor()\n",
    "c_t = LabelClassifier()\n",
    "d_t = DomainClassifier()\n",
    "x_t = torch.Tensor(np.random.randn(5,3,32,32))\n",
    "x_f_t = f_t(x_t)\n",
    "npt.assert_array_equal(x_f_t.shape, (5,128))\n",
    "x_f_t = torch.Tensor(np.random.randn(5,128))\n",
    "x_c_t = c_t(x_f_t)\n",
    "npt.assert_array_equal(x_c_t.shape, (5,10))\n",
    "x_d_t = d_t(x_f_t)\n",
    "npt.assert_array_equal(x_d_t.shape, (5,1))\n",
    "assert torch.all(x_d_t>0) and torch.all(x_d_t<= 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intialize Network Module Objects (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73b1436c245005735f4ccb68efcc96e7",
     "grade": false,
     "grade_id": "initialize_network_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LR = 1e-2 # learning rate\n",
    "LR_decay_rate = 0.999 # learning schedule decay_rate\n",
    "TOTAL_EPOCHS = 5 \n",
    "LOG_PRINT_STEP = 200 # print training progress every LOG_PRINT_STEP iterations\n",
    "\n",
    "# Intialize the following objects for the networks modules\n",
    "# ftr_extr for FeatureExtractor\n",
    "# lbl_clsfr for LabelClassifier\n",
    "# dom_clsfr for DomainClassifier\n",
    "# ftr_extr = \n",
    "# lbl_clsfr = \n",
    "# dom_clsfr = \n",
    "# your code here\n",
    "\n",
    "\n",
    "# Move the network modules to the gpu (if gpu is present)\n",
    "if is_cuda:\n",
    "    ftr_extr = ftr_extr.cuda()\n",
    "    lbl_clsfr = lbl_clsfr.cuda()\n",
    "    dom_clsfr = dom_clsfr.cuda()\n",
    "\n",
    "# Initialize the optimizer and the scheduler for learning rate\n",
    "opt = optim.Adam(list(ftr_extr.parameters()) + list(lbl_clsfr.parameters()), betas=(0.9, 0.999), lr=LR, weight_decay=0.0005)\n",
    "my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=opt, gamma=LR_decay_rate)\n",
    "\n",
    "# Define 'ce_criterion' the nn.CrossEntropyLoss object\n",
    "# This module will implement the softmax and the crossentropy loss\n",
    "# ce_criterion = \n",
    "# your code here\n",
    "\n",
    "\n",
    "print(\"-----------------------------------------FeatrureExtractor---------------------------------------\")\n",
    "print(ftr_extr)\n",
    "print(\"\\n------------------------------------------LabelClassifier------------------------------------------\")\n",
    "print(lbl_clsfr)\n",
    "print(\"\\n------------------------------------------CrossEntropyLoss------------------------------------------\")\n",
    "print(ce_criterion)\n",
    "print(\"\\n------------------------------------------DomainClassifier------------------------------------------\")\n",
    "print(dom_clsfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fba3e5bcc0eda22c3b04ba01c67044c8",
     "grade": true,
     "grade_id": "initialize_network",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Testcases\n",
    "assert isinstance(ftr_extr, FeatureExtractor)\n",
    "assert isinstance(lbl_clsfr, LabelClassifier)\n",
    "assert isinstance(dom_clsfr, DomainClassifier)\n",
    "assert isinstance(ce_criterion, nn.CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1bf6b3784f3b363e79bb45a09a3e951",
     "grade": false,
     "grade_id": "cell-1ea8925eef0263e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Classifier Logits (5 points)\n",
    "\n",
    "Define a function for forward pass through the FeatureExtractor and the LabelClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f392dc56b233a840c740f50ec077fde",
     "grade": false,
     "grade_id": "calc_logits_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_clf_logits(x):\n",
    "    '''\n",
    "    The function propagates input x through the FeatureExtractor and the LabelClassifier\n",
    "    Inputs: \n",
    "        x: Tensor of input images of dimensions (m,3,s,s), where m is number of samples, \n",
    "            3 is for the RGB channels, s is the image size\n",
    "            \n",
    "    Outputs:\n",
    "        logits: Tensor of putputs logits of dimensions (m,10), where m is number of samples,\n",
    "            10 is number of categories\n",
    "    '''\n",
    "    # your code here\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c16276cd684564fab4d0389ef16538b",
     "grade": true,
     "grade_id": "calc_logits",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Testcases\n",
    "# The following testcases test only for dimension constraints. \n",
    "x_t = torch.Tensor(np.random.randn(5,3,32,32))\n",
    "if is_cuda:\n",
    "    x_t = x_t.cuda()\n",
    "npt.assert_array_equal(calc_clf_logits(x_t).shape, (5,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be4b2eb801c2aef622e98dee61fbb96a",
     "grade": false,
     "grade_id": "cell-841ade326ac5aff7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Label Classifier Loss (5 points)\n",
    "\n",
    "Define a function that gives gives the classification loss for the LabelClassifier module. \n",
    "The input is a Tensor of images along with corresponding labels. The images are input to the FeatureExtractor\n",
    "followed by the LabelClassifier and the loss is calculated using CrossEntropy \n",
    "Use the 'calc_clf_logits()' function and the 'ce_criterion' nn.CrossEntropyLoss Object to calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb79ab7588209a85a5e1710f6724b528",
     "grade": false,
     "grade_id": "src_clf_loss_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def src_clf_loss(img, Y):\n",
    "    '''\n",
    "    The function returns the CrossEntropyLoss error for classifying img where groundtruth is labels\n",
    "    Inputs: \n",
    "        img: Tensor of input images of dimensions (m,3,s,s), where m is number of samples, \n",
    "                3 is for the RGB channels, s is the image size\n",
    "        Y: groundtruth labels in Tensor of shape (m) with values in {0,1,,2, ..., 9}\n",
    "            \n",
    "    Outputs:\n",
    "        clf_loss: scalar loss for the m images\n",
    "    '''\n",
    "    # your code here\n",
    "    \n",
    "    return clf_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f097c16a92f71fd0581ce365028ede99",
     "grade": true,
     "grade_id": "src_clf_loss",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Testcases\n",
    "# The following testcases test only for dimension constraints. \n",
    "x_t = torch.Tensor(np.random.randn(5,3,32,32))\n",
    "x_t.requires_grad = True\n",
    "label_t = torch.empty(5, dtype=torch.long).random_(10)\n",
    "if is_cuda:\n",
    "    x_t = x_t.cuda()\n",
    "    label_t = label_t.cuda()\n",
    "out_t = src_clf_loss(x_t, label_t)\n",
    "npt.assert_array_equal(out_t.shape, (1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model\n",
    "\n",
    "Define a function 'evaluate_model(.)' that returns the accuracy of classification for given input images X and groundtruth labels Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bc83d83e5490755ffef816e49b6b555",
     "grade": false,
     "grade_id": "Test_DB",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(X, Y):\n",
    "    '''\n",
    "    The function returns the accuracy of classification\n",
    "    Inputs: \n",
    "        X: Tensor of input images of dimensions (m,3,s,s), where m is number of samples, \n",
    "                3 is for the RGB channels, s is the image size\n",
    "        Y: groundtruth labels in Tensor of shape (m) with values in {0,1,,2, ..., 9}\n",
    "            \n",
    "    Outputs:\n",
    "        acc: accuracy of classification\n",
    "    '''   \n",
    "    ftr_extr.eval()\n",
    "    lbl_clsfr.eval()\n",
    "    actual = []\n",
    "    pred = []\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    for ii in range((m - 1) // BATCH_SIZE + 1):\n",
    "        img = X[ii*BATCH_SIZE : (ii+1)*BATCH_SIZE, :]\n",
    "        label = Y[ii*BATCH_SIZE : (ii+1)*BATCH_SIZE]\n",
    "        if is_cuda:\n",
    "            img = img.cuda()\n",
    "            \n",
    "        # use calc_clf_logits(.) with 'img' as inputs ti get the logits\n",
    "        # Estimate the index of the max value in every row of the logits output, that is the 'predicted' label\n",
    "        logits = calc_clf_logits(img)\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        actual += label.tolist()\n",
    "        pred += predicted.tolist()\n",
    "    acc = accuracy_score(y_true=actual, y_pred=pred) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "164913b14d2bf582227423c9b04cce73",
     "grade": false,
     "grade_id": "cell-c221cb527fc1cf55",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Train Classifier Model With Source Data (5 points)\n",
    "\n",
    "We will train just FeatureExtractor and LabelClassifier with the source data and evaluate classification accuracies on the target data. The trained model is called 'Source-only'. We will train the FeatureExtractor and the LabelClassifier modules using the source data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86dc0c757c93db9a1bf3f211af655caa",
     "grade": true,
     "grade_id": "source_only",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Iterations per epoch: %d\"%(src_trX.shape[0]//BATCH_SIZE))\n",
    "lbl_clsfr.train()\n",
    "ftr_extr.train()\n",
    "m = src_trX.shape[0]\n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    for ii in range((m - 1) // BATCH_SIZE + 1):\n",
    "        s_img = src_trX[ii*BATCH_SIZE : (ii+1)*BATCH_SIZE, :]\n",
    "        s_labels = src_trY[ii*BATCH_SIZE : (ii+1)*BATCH_SIZE]\n",
    "        \n",
    "        if is_cuda:\n",
    "            s_img, s_labels = s_img.cuda(), s_labels.cuda()\n",
    "        \n",
    "        clf_loss = src_clf_loss(s_img, s_labels)\n",
    "        loss = clf_loss\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        my_lr_scheduler.step()\n",
    "                \n",
    "        if ii % LOG_PRINT_STEP == 0:\n",
    "            print(\"Epoch: %d/%d, iter: %4d, clf_err: %.4f, clf_LR: %.3E\" \\\n",
    "                  %(epoch+1, TOTAL_EPOCHS, ii, clf_loss, opt.param_groups[0]['lr']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8138d445b011156c28cff32069a2884c",
     "grade": false,
     "grade_id": "cell-08e602019408207d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Evaluate Classification Accuracies With Source-only model (10 points)\n",
    "\n",
    "Calculate accuracy on source and target datasets. Notice how the Network does not perform well on the target data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c65f6e96cae27ae3cbe2d27f92d02bf7",
     "grade": true,
     "grade_id": "src_acc_tests",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "src_train_acc = evaluate_model(src_trX, src_trY)\n",
    "src_test_acc = evaluate_model(src_tsX, src_tsY)\n",
    "tgt_train_acc = evaluate_model(tgt_trX, tgt_trY)\n",
    "tgt_test_acc = evaluate_model(tgt_tsX, tgt_tsY)\n",
    "print(\"Source train acc: %.2f\\nSource test acc: %.2f\\nTarget train acc: %.2f\\nTarget test acc: %.2f\" \\\n",
    "      %(src_train_acc, src_test_acc, tgt_train_acc, tgt_test_acc))\n",
    "\n",
    "#Testing for source test acc > 90%\n",
    "assert src_train_acc > 90\n",
    "assert src_test_acc > 90\n",
    "assert src_train_acc > tgt_train_acc \n",
    "assert src_train_acc > tgt_test_acc \n",
    "assert src_test_acc > tgt_train_acc \n",
    "assert src_test_acc > tgt_test_acc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a820bf0c6818c73758f0040bea33089",
     "grade": false,
     "grade_id": "cell-2c24c25173837bde",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Intialize the Domain Alignment network  \n",
    "\n",
    "Reintalize the network with same paramters for DANN training. This time we will use the DomainClassifier to align the domains in the DANN style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad29d3caa52b61e22e1e9f39106dbdff",
     "grade": false,
     "grade_id": "reinit_models",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "################### DO NOT MODIFY THE CODE BELOW #############################    \n",
    "##############################################################################\n",
    "manual_seed = 0\n",
    "\n",
    "random.seed(manual_seed)\n",
    "np.random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(manual_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    \n",
    "############################################################################### \n",
    "LR = 1e-2\n",
    "LR_decay_rate = 0.999\n",
    "disc_LR = 1e-4\n",
    "disc_LR_decay_rate = 0.999\n",
    "TOTAL_EPOCHS = 5\n",
    "LOG_PRINT_STEP = 200\n",
    "\n",
    "# Intialize the following objects for the network modules\n",
    "# ftr_extr for FeatureExtractor\n",
    "# lbl_clsfr for LabelClassifier\n",
    "# dom_clsfr for DomainClassifier\n",
    "ftr_extr = FeatureExtractor()\n",
    "lbl_clsfr = LabelClassifier()\n",
    "dom_clsfr = DomainClassifier()\n",
    "\n",
    "# Move the network modules to the gpu (if gpu is present)\n",
    "if is_cuda:\n",
    "    ftr_extr = ftr_extr.cuda()\n",
    "    lbl_clsfr = lbl_clsfr.cuda()\n",
    "    dom_clsfr = dom_clsfr.cuda()\n",
    "\n",
    "# Initialize the optimizer for ftr_extr and lbl_clsfr \n",
    "opt = optim.Adam(list(ftr_extr.parameters()) + list(lbl_clsfr.parameters()), lr=LR, betas=[0.9, 0.999], weight_decay=0.0005)\n",
    "# Setup the learning rate scheduler for the 'opt'\n",
    "my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=opt, gamma=LR_decay_rate)\n",
    "\n",
    "# Initialize the optimizer for dom_clsfr\n",
    "optD = optim.Adam(dom_clsfr.parameters(), lr=disc_LR, betas=(0.9, 0.999), weight_decay=0.0005)\n",
    "# We will not use a scheduler for the 'optD'\n",
    "#my_lr_scheduler_D = torch.optim.lr_scheduler.ExponentialLR(optimizer=optD, gamma=disc_LR_decay_rate)\n",
    "\n",
    "# define the crossentropyloss for LabelClassifier  and the \n",
    "ce_criterion = nn.CrossEntropyLoss()\n",
    "# define the BinaryCrossentropyloss for the DomainClassifier\n",
    "bce_criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3e6d60517746873d79d2b97f9ff4baf",
     "grade": false,
     "grade_id": "cell-8dcb079cffe3a0aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Importance of Domain Alignment (5 points)\n",
    "\n",
    "The parameter $\\lambda$ or 'lam' in our code, controls the importance of the DomainClassifier in the overall objective of domain alignment. The DomainClassifier gradient can conflict with the training of the LabelClassifier with noisy gradients in the initial stages of the training. We therefore set $\\lambda=0$ initially and gradually increase it after the FeatureExtractor has stabilised. This scheduling of 'lam' is implemented in the following 'adjust_lambda' function. \n",
    "Refer to Sec 5.2.2 in the [DANN paper](https://arxiv.org/pdf/1505.07818.pdf) for more details. \n",
    "\n",
    "\\begin{equation}\n",
    "\\lambda = \\frac{2}{1 + \\text{exp}(-\\gamma.p)} -1\n",
    "\\end{equation}\n",
    "where, $\\gamma = 10$ and $p = \\frac{itr + epoch\\times no\\_itrs\\_per\\_epoch}{n\\_epochs \\times no\\_itrs\\_per\\_epoch}$ is the fraction of total iterations completed in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b3613581ebb151cf14f428209493a7b",
     "grade": false,
     "grade_id": "adjust_lambda",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def adjust_lambda(itr, epoch, no_itrs_per_epoch, n_epochs):\n",
    "    '''\n",
    "    returns the scheduled value of lam based on epoch and iteration number. \n",
    "    \n",
    "    Inputs:\n",
    "        itr: the iteration number in an epoch \n",
    "        epoch: the epoch number\n",
    "        no_itrs_per_epoch: the number of iterations (number of mini-batches) in an epoch\n",
    "        n_epochs: total number of epochs in the training\n",
    "    '''\n",
    "    # your code here\n",
    "    \n",
    "    return lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8fd527941224383d0b45368fc692bfe2",
     "grade": true,
     "grade_id": "adjust_lambda_tests",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Test\n",
    "i = 1\n",
    "epoch = 4\n",
    "min_len = 100\n",
    "nepochs = 10\n",
    "lam = adjust_lambda(i, epoch, min_len, nepochs)\n",
    "npt.assert_almost_equal(lam, 0.9643791367189494,decimal=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "957b55a0be4dbbce658fcfb4224d4ce7",
     "grade": false,
     "grade_id": "cell-14fb41df7b0da94a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Domain Classification Loss (10 points)\n",
    "\n",
    "Similar to the Label Classifier Loss function defined in 'src_clf_loss(.)', we will define a Domain Classifier Loss function 'domain_clf_err(.)' which will return the loss of domain classification. \n",
    "The input is a Tensor of source images 's_img', a Tensor of target images 't_img' and the $\\lambda$ parameter 'lam'. We need to create the labels for the groundtruth. The source images belong to class 1 and target images to class 0. \n",
    "Propagate 's_img' through the FeatureExtractor and 't_img' through the FeatureExtractor. Concatenate the outputs and propagate the concatenated result through the DomainClasssifier to obtain the logits. \n",
    "Apply the 'bce_criterion()' defined earlier to determine the loss based on the logits and the groundtruth. \n",
    "The 'bce_criterion' is similar to 'ce_criterion' for CrossEntropyLoss\n",
    "\n",
    "It may not be possible to concatenate 's_img' and 't_img' and forward propagate the concatenated output through the FeatureExtractor and DomainClassifier - there may be a dimension mismatch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88c46dc8200f406aa5f936addf872f22",
     "grade": false,
     "grade_id": "dann_loss_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def domain_clf_loss(s_img, t_img, lam):\n",
    "    '''\n",
    "    The function returns the BinaryCrosEntropyLoss trying to distingiish between the source images 's_img'\n",
    "    from the target images 't_img'\n",
    "    Inputs: \n",
    "        s_img: Tensor of source input images of dimensions (m,3,32,32), where m is number of samples, \n",
    "                3 is for the RGB channels, 32 is the image size\n",
    "        t_img: Tensor of target input images of dimensions (m,3,28,28), where m is number of samples, \n",
    "                3 is for the RGB channels, 28 is the image size\n",
    "        lam: lambda parameter controlling the importance of domain alignment\n",
    "            \n",
    "    Outputs:\n",
    "        dom_loss: scalar loss of domain classification\n",
    "    '''\n",
    "    # Generate the groundtruth labels 1 for source and 0 for target\n",
    "    # Concatenate the groundtruth labels to get 'labels' and move 'labels' to cuda() if is_cuda\n",
    "    # Concatenate the output of 'ftr_extr(s_img) using 's_img' and 'ftr_extr(t_img) using 't_img' to get 'imgs'\n",
    "    # and move 'imgs' to cuda if is_cuda\n",
    "    # Forward propagate 'imgs' through 'dom_clsfr(.)' using 'lam' to get the logits for domain classification\n",
    "    # Estimate domain classification loss 'dom_loss' by comparing the logits with the groundtruth using bce_criterion(.)\n",
    "    # your code here\n",
    "    \n",
    "    return dom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0813265200e1ad19b440a8d11e3957d",
     "grade": true,
     "grade_id": "dann_loss",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Testcases\n",
    "# The following testcases test only for dimension constraints. \n",
    "xs_t = torch.Tensor(np.random.randn(5,3,32,32))\n",
    "xt_t = torch.Tensor(np.random.randn(5,3,32,32))\n",
    "if is_cuda:\n",
    "    xs_t = xs_t.cuda()\n",
    "    xt_t = xt_t.cuda()\n",
    "out_t = domain_clf_loss(xs_t, xt_t, lam=1.)\n",
    "npt.assert_array_equal(out_t.shape, (1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa152ea5d48d02e5d368547c9b72b60a",
     "grade": false,
     "grade_id": "cell-75cf35bc60f8cdd0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Train the DANN Network (10 points)\n",
    "\n",
    "DANN training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13715e4bf4337f7ce59730b405fa73d8",
     "grade": true,
     "grade_id": "dann_train",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "max_len = max(src_trX.shape[0], tgt_trX.shape[0])\n",
    "print(\"Iterations per epoch: %d\"%(max_len//BATCH_SIZE))\n",
    "lbl_clsfr.train()\n",
    "ftr_extr.train()\n",
    "dom_clsfr.train()\n",
    "\n",
    "src_bigger = False\n",
    "#repeat indices since datasets are of different sizes\n",
    "src_idx = range((src_trX.shape[0] - 1) // BATCH_SIZE + 1)\n",
    "tgt_idx = range((tgt_trX.shape[0] - 1) // BATCH_SIZE + 1)\n",
    "if src_trX.shape[0] > tgt_trX.shape[0]:\n",
    "    tgt_idx = np.resize(tgt_idx, src_trX.shape[0])\n",
    "    src_bigger = True\n",
    "else:\n",
    "    src_idx = np.resize(src_idx, tgt_trX.shape[0])\n",
    "    \n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    for ii, jj in zip(src_idx, tgt_idx):\n",
    "        s_img = src_trX[ii*BATCH_SIZE : (ii+1)*BATCH_SIZE, :]\n",
    "        s_labels = src_trY[ii*BATCH_SIZE : (ii+1)*BATCH_SIZE]\n",
    "        t_img = tgt_trX[jj*BATCH_SIZE : (jj+1)*BATCH_SIZE, :]\n",
    "        t_labels = tgt_trY[jj*BATCH_SIZE : (jj+1)*BATCH_SIZE]\n",
    "        \n",
    "        if src_bigger:\n",
    "            lam = adjust_lambda(ii, epoch, max_len//BATCH_SIZE, TOTAL_EPOCHS)\n",
    "        else:\n",
    "            lam = adjust_lambda(jj, epoch, max_len//BATCH_SIZE, TOTAL_EPOCHS)\n",
    "            \n",
    "        if is_cuda:\n",
    "            s_img, s_labels, t_img, t_labels = s_img.cuda(), s_labels.cuda(), t_img.cuda(), t_labels.cuda()\n",
    "        \n",
    "        clf_loss = src_clf_loss(s_img, s_labels)\n",
    "        \n",
    "        dom_loss = domain_clf_loss(s_img, t_img, lam)\n",
    "                  \n",
    "        loss = clf_loss + dom_loss\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        optD.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        optD.step()\n",
    "        my_lr_scheduler.step()\n",
    "        #my_lr_scheduler_D.step()\n",
    "        \n",
    "        if src_bigger:\n",
    "            if ii % LOG_PRINT_STEP == 0:\n",
    "                print(\"Epoch: %d/%d, iter: %4d, lambda: %.2f, clf_loss: %.4f, clf_LR: %.3E, dom_loss: %.4f, dom_LR: %.3E\"\\\n",
    "                      %(epoch+1, TOTAL_EPOCHS, ii, lam, clf_loss, opt.param_groups[0]['lr'], dom_loss, optD.param_groups[0]['lr']))\n",
    "        else:\n",
    "            if jj % LOG_PRINT_STEP == 0:\n",
    "                print(\"Epoch: %d/%d, iter: %4d, lambda: %.2f, clf_err: %.4f, clf_LR: %.3E, disc_err: %.4f, dom_LR: %.3E\"\\\n",
    "                      %(epoch+1, TOTAL_EPOCHS, jj, lam, clf_loss, opt.param_groups[0]['lr'], dom_loss, optD.param_groups[0]['lr']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e73912091f91634709211a667ee1ddbf",
     "grade": false,
     "grade_id": "cell-63104ce21d2f3102",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Evaluate the DANN Network (20 points)\n",
    "\n",
    "Calculate accuracy on source and target datasets using DANN. Observe the improvement in the target classification accuracies with the alignment of the source and target datasets. You should observe the accuracy of the target increase by at least 10 points compared to Source-only training. The settings mentioned here yielded a target test accuracy of 71%. \n",
    "Modify the network and the training procedure to the extent allowed to improve the target classification accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4fcdadbf12d26eac1035675ac175489",
     "grade": true,
     "grade_id": "dann_acc_tests",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "src_train_acc2 = evaluate_model(src_trX, src_trY)\n",
    "src_test_acc2 = evaluate_model(src_tsX, src_tsY)\n",
    "tgt_train_acc2 = evaluate_model(tgt_trX, tgt_trY)\n",
    "tgt_test_acc2 = evaluate_model(tgt_tsX, tgt_tsY)\n",
    "print(\"With Domain Adversarial Training:\\nSource train acc: %.2f\\nSource test acc: %.2f\\nTarget train acc: %.2f\\nTarget test acc: %.2f\" \\\n",
    "      %(src_train_acc2, src_test_acc2, tgt_train_acc2, tgt_test_acc2))\n",
    "\n",
    "# Hidden testcases follow \n",
    "# Test improvement of target accuracy by at least 10 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15bdcb13a55787dd2256f032294d0a2f",
     "grade": false,
     "grade_id": "cell-790848acb456f8d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### The assignment is graded both manually and using autograding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
